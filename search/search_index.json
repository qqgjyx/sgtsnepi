{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"PySGtSNEpi","text":"<p>Embed sparse graphs into 2D/3D \u2014 pure Python, pip-installable, sklearn-compatible.</p> <p>PySGtSNEpi is a pure Python port of the SG-t-SNE-Pi algorithm, translated from the original C++ and Julia implementations. Unlike standard t-SNE, SG-t-SNE-Pi works on any sparse stochastic graph, not just kNN graphs derived from point clouds. No C/C++ compiler needed \u2014 <code>pip install</code> and go.</p>"},{"location":"index.html#features","title":"Features","text":"<ul> <li>1D / 2D / 3D embedding of sparse stochastic graphs</li> <li>Arbitrary sparse graph input \u2014 not limited to kNN graphs</li> <li>Point cloud input with automatic kNN graph construction via PyNNDescent</li> <li>Lambda rescaling to equalize effective node degrees</li> <li>Scikit-learn compatible API (<code>fit</code> / <code>transform</code> / <code>fit_transform</code>)</li> <li>Pure Python \u2014 runs on Windows, macOS (including Apple Silicon), and Linux</li> <li>Numba JIT compiled hot loops for near-native speed</li> <li>FFT-accelerated repulsive force computation</li> </ul>"},{"location":"index.html#quick-start","title":"Quick Start","text":"<pre><code>pip install pysgtsnepi\n</code></pre>"},{"location":"index.html#scikit-learn-api-point-cloud","title":"Scikit-learn API (point cloud)","text":"<pre><code>from pysgtsnepi import SGtSNEpi\n\nmodel = SGtSNEpi(d=2, lambda_=10)\nY = model.fit_transform(X)   # X is (n_samples, n_features)\n</code></pre>"},{"location":"index.html#functional-api-sparse-graph","title":"Functional API (sparse graph)","text":"<pre><code>from scipy.io import mmread\nfrom pysgtsnepi import sgtsnepi\n\nP = mmread(\"graph.mtx\")       # sparse stochastic graph\nY = sgtsnepi(P, d=3, lambda_=10)\n</code></pre>"},{"location":"index.html#roadmap","title":"Roadmap","text":"<ul> <li>[x] Lambda equalization</li> <li>[x] kNN graph construction (via PyNNDescent)</li> <li>[x] Core SG-t-SNE-Pi embedding (attractive + repulsive forces)</li> <li>[x] FFT-accelerated repulsive forces</li> <li>[x] Numba JIT for interpolation and gradient kernels</li> <li>[x] 1D / 3D embedding support</li> <li>[x] <code>SGtSNEpi</code> sklearn estimator class</li> <li>[x] <code>sgtsnepi()</code> functional API</li> </ul>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use this package in your research, please cite:</p> <pre><code>@article{pitsianis2019joss,\n  title     = {{SG-t-SNE-$\\Pi$}: Swift Neighbor Embedding of Sparse Stochastic Graphs},\n  author    = {Pitsianis, Nikos and Floros, Dimitris and Iliopoulos, Alexandros-Stavros and Sun, Xiaobai},\n  journal   = {Journal of Open Source Software},\n  volume    = {4},\n  number    = {39},\n  pages     = {1577},\n  year      = {2019},\n  doi       = {10.21105/joss.01577}\n}\n\n@inproceedings{pitsianis2019hpec,\n  title     = {Spaceland Embedding of Sparse Stochastic Graphs},\n  author    = {Pitsianis, Nikos and Iliopoulos, Alexandros-Stavros and Floros, Dimitris and Sun, Xiaobai},\n  booktitle = {IEEE High Performance Extreme Computing Conference},\n  year      = {2019},\n  doi       = {10.1109/HPEC.2019.8916505}\n}\n</code></pre>"},{"location":"index.html#links","title":"Links","text":"<ul> <li>Algorithm website</li> <li>C++ implementation</li> <li>Julia implementation</li> <li>Documentation</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>MIT \u2014 see LICENSE.</p>"},{"location":"install.html","title":"\ud83d\udce6 Installation","text":""},{"location":"install.html#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install pysgtsnepi\n</code></pre>"},{"location":"install.html#install-from-source","title":"Install from source","text":"<pre><code>git clone https://github.com/qqgjyx/sgtsnepi.git\ncd sgtsnepi\npip install .\n</code></pre>"},{"location":"api/index.html","title":"API Reference","text":""},{"location":"api/index.html#core","title":"Core","text":"Module Description <code>sgtsnepi</code> Functional API \u2014 <code>sgtsnepi(P, d=2, ...)</code> <code>SGtSNEpi</code> Sklearn-compatible estimator class"},{"location":"api/index.html#pipeline","title":"Pipeline","text":"Module Description <code>knn</code> kNN graph construction via PyNNDescent <code>attractive</code> Sparse attractive gradient computation <code>repulsive</code> FFT-accelerated repulsive forces <code>embedding</code> Optimizer loop (Adam, early exaggeration)"},{"location":"api/index.html#utilities","title":"Utilities","text":"Module Description <code>graph_rescaling</code> Lambda-based graph rescaling"},{"location":"api/attractive.html","title":"<code>attractive</code> \u2014 Attractive Forces","text":"<p>Attractive forces for SG-t-SNE-Pi (sparse PQ component).</p>"},{"location":"api/attractive.html#pysgtsnepi.attractive.attractive_forces","title":"<code>attractive_forces(P, Y)</code>","text":"<p>Compute attractive forces from sparse probability matrix P.</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <code>csc_matrix of shape (n, n)</code> <p>Sparse probability matrix.</p> required <code>Y</code> <code>ndarray of shape (n, d)</code> <p>Current embedding coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (n, d)</code> <p>Attractive force on each point.</p> Source code in <code>src/pysgtsnepi/attractive.py</code> <pre><code>def attractive_forces(P: csc_matrix, Y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute attractive forces from sparse probability matrix P.\n\n    Parameters\n    ----------\n    P : csc_matrix of shape (n, n)\n        Sparse probability matrix.\n    Y : ndarray of shape (n, d)\n        Current embedding coordinates.\n\n    Returns\n    -------\n    ndarray of shape (n, d)\n        Attractive force on each point.\n    \"\"\"\n    n, d = Y.shape\n    return _attractive_forces_kernel(\n        P.indices, P.indptr, P.data, np.ascontiguousarray(Y), n, d\n    )\n</code></pre>"},{"location":"api/embedding.html","title":"<code>embedding</code> \u2014 Embedding Loop","text":"<p>SG-t-SNE-Pi gradient descent optimization loop.</p> <p>Translated from ref/sgtsnepi/src/gradient_descend.cpp.</p>"},{"location":"api/embedding.html#pysgtsnepi.embedding.sgtsne_embedding","title":"<code>sgtsne_embedding(P, d=2, max_iter=1000, early_exag=250, alpha=12.0, eta=200.0, h=1.0, Y0=None, random_state=None)</code>","text":"<p>Run SG-t-SNE-Pi gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <code>csc_matrix</code> <p>Symmetrized, normalized probability matrix (n, n).</p> required <code>d</code> <code>int</code> <p>Embedding dimensions (1, 2, or 3).</p> <code>2</code> <code>max_iter</code> <code>int</code> <p>Total iterations.</p> <code>1000</code> <code>early_exag</code> <code>int</code> <p>Number of early exaggeration iterations.</p> <code>250</code> <code>alpha</code> <code>float</code> <p>Exaggeration multiplier (drops to 1 after early_exag).</p> <code>12.0</code> <code>eta</code> <code>float</code> <p>Learning rate.</p> <code>200.0</code> <code>h</code> <code>float</code> <p>Grid side length for FFT.</p> <code>1.0</code> <code>Y0</code> <code>ndarray or None</code> <p>Initial embedding of shape (n, d). If None, random init with scale 0.01 (matching Julia tutorial convention).</p> <code>None</code> <code>random_state</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Y</code> <code>ndarray of shape (n, d)</code> <p>Final embedding coordinates.</p> Source code in <code>src/pysgtsnepi/embedding.py</code> <pre><code>def sgtsne_embedding(\n    P: csc_matrix,\n    d: int = 2,\n    max_iter: int = 1000,\n    early_exag: int = 250,\n    alpha: float = 12.0,\n    eta: float = 200.0,\n    h: float = 1.0,\n    Y0: np.ndarray | None = None,\n    random_state: int | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Run SG-t-SNE-Pi gradient descent.\n\n    Parameters\n    ----------\n    P : csc_matrix\n        Symmetrized, normalized probability matrix (n, n).\n    d : int\n        Embedding dimensions (1, 2, or 3).\n    max_iter : int\n        Total iterations.\n    early_exag : int\n        Number of early exaggeration iterations.\n    alpha : float\n        Exaggeration multiplier (drops to 1 after early_exag).\n    eta : float\n        Learning rate.\n    h : float\n        Grid side length for FFT.\n    Y0 : ndarray or None\n        Initial embedding of shape (n, d). If None, random init\n        with scale 0.01 (matching Julia tutorial convention).\n    random_state : int or None\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    Y : ndarray of shape (n, d)\n        Final embedding coordinates.\n    \"\"\"\n    n = P.shape[0]\n    rng = np.random.default_rng(random_state)\n\n    # Initialize embedding\n    if Y0 is not None:\n        Y = Y0.copy().astype(np.float64)\n    else:\n        Y = rng.standard_normal((n, d)) * 0.01\n\n    # Momentum parameters (same as C++ reference)\n    momentum = 0.5\n    final_momentum = 0.8\n    mom_switch_iter = 250\n\n    # Allocate state\n    uY = np.zeros((n, d), dtype=np.float64)\n    gains = np.ones((n, d), dtype=np.float64)\n\n    current_alpha = alpha\n\n    for it in range(max_iter):\n        # Compute attractive forces\n        Fattr = attractive_forces(P, Y)\n\n        # Compute repulsive forces\n        Frep, _zeta = compute_repulsive_forces(Y, h)\n\n        # Gradient: dY = alpha * Fattr - Frep\n        dY = current_alpha * Fattr - Frep\n\n        # Gain adaptation\n        gains = np.where(np.sign(dY) != np.sign(uY), gains + 0.2, gains * 0.8)\n        np.clip(gains, 0.01, None, out=gains)\n\n        # Velocity update\n        uY = momentum * uY - eta * gains * dY\n\n        # Position update\n        Y += uY\n\n        # Zero-mean center\n        Y -= Y.mean(axis=0)\n\n        # Stop early exaggeration\n        if it == early_exag:\n            current_alpha = 1.0\n\n        # Switch momentum\n        if it == mom_switch_iter:\n            momentum = final_momentum\n\n    return Y\n</code></pre>"},{"location":"api/estimator.html","title":"<code>SGtSNEpi</code> \u2014 Sklearn Estimator","text":"<p>Sklearn-compatible estimator for SG-t-SNE-Pi.</p>"},{"location":"api/estimator.html#pysgtsnepi.estimator.SGtSNEpi","title":"<code>SGtSNEpi</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>SG-t-SNE-Pi graph embedding.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>int</code> <p>Embedding dimensions (default 2).</p> <code>2</code> <code>lambda_</code> <code>float</code> <p>Rescaling parameter (default 10).</p> <code>10.0</code> <code>n_neighbors</code> <code>int</code> <p>Number of nearest neighbors for kNN (default 15).</p> <code>15</code> <code>metric</code> <code>str</code> <p>Distance metric (default \"euclidean\").</p> <code>'euclidean'</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations (default 1000).</p> <code>1000</code> <code>early_exag</code> <code>int</code> <p>Early exaggeration iterations (default 250).</p> <code>250</code> <code>alpha</code> <code>float</code> <p>Exaggeration multiplier (default 12).</p> <code>12.0</code> <code>eta</code> <code>float</code> <p>Learning rate (default 200).</p> <code>200.0</code> <code>h</code> <code>float</code> <p>Grid side length for FFT. 0 = auto.</p> <code>0.0</code> <code>random_state</code> <code>int or None</code> <p>Random seed.</p> <code>None</code> <code>unweighted_to_weighted</code> <code>bool</code> <p>If True (default) and the input graph is unweighted, compute Jaccard-index local weights before embedding.</p> <code>True</code> Source code in <code>src/pysgtsnepi/estimator.py</code> <pre><code>class SGtSNEpi(BaseEstimator, TransformerMixin):\n    \"\"\"SG-t-SNE-Pi graph embedding.\n\n    Parameters\n    ----------\n    d : int\n        Embedding dimensions (default 2).\n    lambda_ : float\n        Rescaling parameter (default 10).\n    n_neighbors : int\n        Number of nearest neighbors for kNN (default 15).\n    metric : str\n        Distance metric (default \"euclidean\").\n    max_iter : int\n        Maximum iterations (default 1000).\n    early_exag : int\n        Early exaggeration iterations (default 250).\n    alpha : float\n        Exaggeration multiplier (default 12).\n    eta : float\n        Learning rate (default 200).\n    h : float\n        Grid side length for FFT. 0 = auto.\n    random_state : int or None\n        Random seed.\n    unweighted_to_weighted : bool\n        If True (default) and the input graph is unweighted, compute\n        Jaccard-index local weights before embedding.\n    \"\"\"\n\n    def __init__(\n        self,\n        d: int = 2,\n        lambda_: float = 10.0,\n        n_neighbors: int = 15,\n        metric: str = \"euclidean\",\n        max_iter: int = 1000,\n        early_exag: int = 250,\n        alpha: float = 12.0,\n        eta: float = 200.0,\n        h: float = 0.0,\n        random_state: int | None = None,\n        unweighted_to_weighted: bool = True,\n    ):\n        self.d = d\n        self.lambda_ = lambda_\n        self.n_neighbors = n_neighbors\n        self.metric = metric\n        self.max_iter = max_iter\n        self.early_exag = early_exag\n        self.alpha = alpha\n        self.eta = eta\n        self.h = h\n        self.random_state = random_state\n        self.unweighted_to_weighted = unweighted_to_weighted\n\n    def fit(self, X, y=None):\n        \"\"\"Compute the embedding.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape (n_samples, n_features)\n            If dense, builds kNN graph first.\n            If sparse, treats as adjacency matrix.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.fit_transform(X, y)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Compute and return the embedding.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape (n_samples, n_features)\n            If dense, builds kNN graph first.\n            If sparse, treats as adjacency matrix.\n\n        Returns\n        -------\n        Y : ndarray of shape (n_samples, d)\n        \"\"\"\n        if issparse(X):\n            A = X\n        else:\n            X = np.asarray(X, dtype=np.float64)\n            A = build_knn_graph(\n                X,\n                n_neighbors=self.n_neighbors,\n                metric=self.metric,\n                random_state=self.random_state,\n                perplexity=self.lambda_,\n            )\n\n        Y = sgtsnepi(\n            A,\n            d=self.d,\n            lambda_=self.lambda_,\n            max_iter=self.max_iter,\n            early_exag=self.early_exag,\n            alpha=self.alpha,\n            eta=self.eta,\n            h=self.h,\n            random_state=self.random_state,\n            unweighted_to_weighted=self.unweighted_to_weighted,\n        )\n\n        self.embedding_ = Y\n        return Y\n</code></pre>"},{"location":"api/estimator.html#pysgtsnepi.estimator.SGtSNEpi.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix of shape (n_samples, n_features)</code> <p>If dense, builds kNN graph first. If sparse, treats as adjacency matrix.</p> required <p>Returns:</p> Type Description <code>self</code> Source code in <code>src/pysgtsnepi/estimator.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Compute the embedding.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix of shape (n_samples, n_features)\n        If dense, builds kNN graph first.\n        If sparse, treats as adjacency matrix.\n\n    Returns\n    -------\n    self\n    \"\"\"\n    self.fit_transform(X, y)\n    return self\n</code></pre>"},{"location":"api/estimator.html#pysgtsnepi.estimator.SGtSNEpi.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Compute and return the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix of shape (n_samples, n_features)</code> <p>If dense, builds kNN graph first. If sparse, treats as adjacency matrix.</p> required <p>Returns:</p> Name Type Description <code>Y</code> <code>ndarray of shape (n_samples, d)</code> Source code in <code>src/pysgtsnepi/estimator.py</code> <pre><code>def fit_transform(self, X, y=None):\n    \"\"\"Compute and return the embedding.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix of shape (n_samples, n_features)\n        If dense, builds kNN graph first.\n        If sparse, treats as adjacency matrix.\n\n    Returns\n    -------\n    Y : ndarray of shape (n_samples, d)\n    \"\"\"\n    if issparse(X):\n        A = X\n    else:\n        X = np.asarray(X, dtype=np.float64)\n        A = build_knn_graph(\n            X,\n            n_neighbors=self.n_neighbors,\n            metric=self.metric,\n            random_state=self.random_state,\n            perplexity=self.lambda_,\n        )\n\n    Y = sgtsnepi(\n        A,\n        d=self.d,\n        lambda_=self.lambda_,\n        max_iter=self.max_iter,\n        early_exag=self.early_exag,\n        alpha=self.alpha,\n        eta=self.eta,\n        h=self.h,\n        random_state=self.random_state,\n        unweighted_to_weighted=self.unweighted_to_weighted,\n    )\n\n    self.embedding_ = Y\n    return Y\n</code></pre>"},{"location":"api/knn.html","title":"<code>knn</code> \u2014 kNN Graph Construction","text":"<p>kNN graph construction via PyNNDescent.</p>"},{"location":"api/knn.html#pysgtsnepi.knn.build_knn_graph","title":"<code>build_knn_graph(X, n_neighbors=15, metric='euclidean', random_state=None, perplexity=None)</code>","text":"<p>Build sparse kNN graph from a point cloud.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_features)</code> <p>Input data.</p> required <code>n_neighbors</code> <code>int</code> <p>Number of nearest neighbors (excluding self).</p> <code>15</code> <code>metric</code> <code>str</code> <p>Distance metric for PyNNDescent.</p> <code>'euclidean'</code> <code>random_state</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>perplexity</code> <code>float or None</code> <p>If set, apply perplexity equalization (Gaussian kernel with adaptive bandwidth) and return a column-stochastic probability matrix.  If <code>None</code>, return raw squared distances.</p> <code>None</code> <p>Returns:</p> Type Description <code>csc_matrix</code> <p>Sparse (n_samples, n_samples) matrix.  Column-stochastic probabilities if perplexity is set, raw squared distances otherwise.  Self-loops removed.</p> Source code in <code>src/pysgtsnepi/knn.py</code> <pre><code>def build_knn_graph(\n    X: np.ndarray,\n    n_neighbors: int = 15,\n    metric: str = \"euclidean\",\n    random_state: int | None = None,\n    perplexity: float | None = None,\n) -&gt; csc_matrix:\n    \"\"\"Build sparse kNN graph from a point cloud.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        Input data.\n    n_neighbors : int\n        Number of nearest neighbors (excluding self).\n    metric : str\n        Distance metric for PyNNDescent.\n    random_state : int or None\n        Random seed for reproducibility.\n    perplexity : float or None\n        If set, apply perplexity equalization (Gaussian kernel with\n        adaptive bandwidth) and return a column-stochastic probability\n        matrix.  If ``None``, return raw squared distances.\n\n    Returns\n    -------\n    scipy.sparse.csc_matrix\n        Sparse (n_samples, n_samples) matrix.  Column-stochastic\n        probabilities if *perplexity* is set, raw squared distances\n        otherwise.  Self-loops removed.\n    \"\"\"\n    n = X.shape[0]\n    k = n_neighbors\n\n    # PyNNDescent finds k+1 neighbors (includes self at index 0)\n    index = NNDescent(\n        X,\n        n_neighbors=k + 1,\n        metric=metric,\n        random_state=random_state,\n    )\n    indices, distances = index.neighbor_graph\n\n    # Remove self-neighbor (first column is always self with distance ~0)\n    knn_indices = indices[:, 1:]  # (n, k)\n    knn_distances = distances[:, 1:]  # (n, k)\n\n    # Square distances (algorithm expects D\u00b2)\n    knn_distances = knn_distances**2\n\n    # Build CSC sparse matrix: column j has k entries at rows knn_indices[j,:]\n    row_ind = knn_indices.ravel()\n    col_ind = np.repeat(np.arange(n), k)\n    data = knn_distances.ravel()\n\n    graph = csc_matrix((data, (row_ind, col_ind)), shape=(n, n))\n\n    # Remove any remaining self-loops and explicit zeros\n    graph.setdiag(0)\n    graph.eliminate_zeros()\n\n    # Apply perplexity equalization if requested\n    if perplexity is not None:\n        graph = _perplexity_equalize(graph, perplexity)\n\n    return graph\n</code></pre>"},{"location":"api/lambda-eq.html","title":"<code>graph_rescaling</code>","text":"<p>Lambda-based graph rescaling.</p> <p>Direct translation of C++ <code>graph_rescaling.cpp:lambdaRescaling</code>.</p>"},{"location":"api/lambda-eq.html#pysgtsnepi.utils.graph_rescaling.lambda_rescaling","title":"<code>lambda_rescaling(P, lambda_, tol=1e-05, max_iter=100)</code>","text":"<p>Rescale column-stochastic graph using lambda parameter.</p> <p>For each column j of the input matrix (which must already be column-stochastic):</p> <ol> <li>Convert probabilities to distances: <code>D = -log(P)</code></li> <li>Binary-search for <code>sigma</code> such that    <code>sum(exp(-D * sigma)) == lambda_</code></li> <li>Update values: <code>P_ij = exp(-D_ij * sigma)</code></li> <li>Normalize column to sum to 1</li> </ol> <p>This matches C++ <code>graph_rescaling.cpp:lambdaRescaling</code> (lines 19-103).</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <code>csc_matrix</code> <p>Column-stochastic sparse matrix (values in (0, 1]).</p> required <code>lambda_</code> <code>float</code> <p>Target column sum after rescaling (before normalization).</p> required <code>tol</code> <code>float</code> <p>Convergence tolerance for bisection.</p> <code>1e-05</code> <code>max_iter</code> <code>int</code> <p>Maximum bisection iterations per column.</p> <code>100</code> <p>Returns:</p> Type Description <code>csc_matrix</code> <p>Rescaled column-stochastic matrix.</p> Source code in <code>src/pysgtsnepi/utils/graph_rescaling.py</code> <pre><code>def lambda_rescaling(\n    P: csc_matrix,\n    lambda_: float,\n    tol: float = 1e-5,\n    max_iter: int = 100,\n) -&gt; csc_matrix:\n    \"\"\"Rescale column-stochastic graph using lambda parameter.\n\n    For each column *j* of the input matrix (which must already be\n    column-stochastic):\n\n    1. Convert probabilities to distances: ``D = -log(P)``\n    2. Binary-search for ``sigma`` such that\n       ``sum(exp(-D * sigma)) == lambda_``\n    3. Update values: ``P_ij = exp(-D_ij * sigma)``\n    4. Normalize column to sum to 1\n\n    This matches C++ ``graph_rescaling.cpp:lambdaRescaling`` (lines 19-103).\n\n    Parameters\n    ----------\n    P : csc_matrix\n        Column-stochastic sparse matrix (values in (0, 1]).\n    lambda_ : float\n        Target column sum after rescaling (before normalization).\n    tol : float\n        Convergence tolerance for bisection.\n    max_iter : int\n        Maximum bisection iterations per column.\n\n    Returns\n    -------\n    csc_matrix\n        Rescaled column-stochastic matrix.\n    \"\"\"\n    P = csc_matrix(P, dtype=np.float64, copy=True)\n    n = P.shape[0]\n\n    sigma = np.ones(n)\n    i_diff = np.zeros(n)\n    i_count = np.zeros(n)\n\n    # Convert probabilities to distances in-place: D = -log(P)\n    # Matches C++ line 40: P.val[j] = -log(P.val[j])\n    tiny = np.finfo(np.float64).tiny\n    P.data[:] = -np.log(np.maximum(P.data, tiny))\n\n    for j in range(n):\n        start, end = P.indptr[j], P.indptr[j + 1]\n        if start == end:\n            continue\n\n        vals = P.data[start:end]\n\n        # Initial residual: sum(exp(-D * 1.0)) - lambda\n        lse = logsumexp(-vals)\n        fval = (np.exp(lse) if lse &lt; 700 else float(\"inf\")) - lambda_\n\n        lb = -1e3\n        ub = float(\"inf\")\n        it = 0\n\n        # Bisection search (C++ lines 54-84)\n        while abs(fval) &gt; tol and it &lt; max_iter:\n            it += 1\n\n            if fval &gt; 0:\n                lb = sigma[j]\n                if np.isinf(ub):\n                    sigma[j] = 2 * lb\n                else:\n                    sigma[j] = 0.5 * (lb + ub)\n            else:\n                ub = sigma[j]\n                sigma[j] = 0.5 * (lb + ub)\n\n            lse = logsumexp(-vals * sigma[j])\n            sum_j = np.exp(lse) if lse &lt; 700 else float(\"inf\")\n            if sum_j == 0:\n                sum_j = tiny\n            fval = sum_j - lambda_\n\n        i_diff[j] = fval\n        i_count[j] = it\n\n        # Update values: exp(-D * sigma)  (C++ lines 89-92)\n        new_vals = np.exp(-vals * sigma[j])\n        # Column-stochastic normalization (C++ lines 95-96)\n        col_sum = new_vals.sum()\n        if col_sum &gt; 0:\n            new_vals /= col_sum\n        P.data[start:end] = new_vals\n\n    # Diagnostics\n    avg_iter = int(np.ceil(np.sum(i_count) / max(n, 1)))\n    nc_idx = int(np.sum(np.abs(i_diff) &gt; tol))\n\n    if nc_idx == 0:\n        logger.info(\n            \"All %d elements converged numerically, avg(#iter) = %d\", n, avg_iter\n        )\n    else:\n        warnings.warn(\n            f\"There are {nc_idx} non-convergent elements out of {n}\", stacklevel=2\n        )\n\n    n_neg = int(np.sum(sigma &lt; 0))\n    if n_neg &gt; 0:\n        warnings.warn(\n            f\"There are {n_neg} nodes with negative sigma; consider decreasing lambda\",\n            stacklevel=2,\n        )\n\n    return P\n</code></pre>"},{"location":"api/repulsive.html","title":"<code>repulsive</code> \u2014 Repulsive Forces","text":"<p>FFT-accelerated repulsive forces for SG-t-SNE-Pi.</p> <p>Translated from ref/sgtsnepi/src/qq.cpp, gridding.cpp, nuconv.cpp, non_periodic_conv.cpp.</p>"},{"location":"api/repulsive.html#pysgtsnepi.repulsive.compute_repulsive_forces","title":"<code>compute_repulsive_forces(Y, h)</code>","text":"<p>Compute FFT-accelerated repulsive forces.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>ndarray of shape (n, d)</code> <p>Current embedding coordinates.</p> required <code>h</code> <code>float</code> <p>Grid side length for FFT.</p> required <p>Returns:</p> Name Type Description <code>Frep</code> <code>ndarray of shape (n, d)</code> <p>Repulsive forces.</p> <code>zeta</code> <code>float</code> <p>Normalization constant.</p> Source code in <code>src/pysgtsnepi/repulsive.py</code> <pre><code>def compute_repulsive_forces(Y: np.ndarray, h: float) -&gt; tuple[np.ndarray, float]:\n    \"\"\"Compute FFT-accelerated repulsive forces.\n\n    Parameters\n    ----------\n    Y : ndarray of shape (n, d)\n        Current embedding coordinates.\n    h : float\n        Grid side length for FFT.\n\n    Returns\n    -------\n    Frep : ndarray of shape (n, d)\n        Repulsive forces.\n    zeta : float\n        Normalization constant.\n    \"\"\"\n    n, d = Y.shape\n    eps = np.finfo(np.float64).eps\n\n    # Guard against NaN/Inf input\n    if not np.all(np.isfinite(Y)):\n        return np.zeros((n, d), dtype=np.float64), 1.0\n\n    # Work on copies \u2014 do NOT modify the embedding Y in-place\n    Y_shifted = Y - Y.min(axis=0)\n\n    # Global max across all dims\n    maxy = Y_shifted.max()\n    if maxy &lt; eps:\n        return np.zeros((n, d), dtype=np.float64), 1.0\n\n    # Grid size\n    n_grid = max(math.ceil(maxy / h), 14)\n    n_grid = _best_grid_size(n_grid)\n\n    # Grid positions in [0, n_grid-1]\n    Y_grid = Y_shifted / maxy * (n_grid - 1)\n    # Clamp to avoid edge issues\n    Y_grid = np.clip(Y_grid, 0, n_grid - 1 - eps)\n\n    # Exact grid spacing in data space\n    h_exact = maxy / (n_grid - 1 - eps)\n\n    ng = n_grid + 3  # padded grid: 4-point stencil at pos n_grid-1 needs idx n_grid+2\n    n_vec = d + 1  # number of value channels\n\n    # Setup scattered values: VScat[:, 0] = 1, VScat[:, 1:] = Y_shifted\n    VScat = np.empty((n, n_vec), dtype=np.float64)\n    VScat[:, 0] = 1.0\n    VScat[:, 1:] = Y_shifted\n\n    # Ensure grid coordinates are 2D for the numba kernels\n    Y_grid_2d = np.ascontiguousarray(Y_grid.reshape(n, d))\n\n    # S2G: scatter to grid\n    grid_shape = tuple([ng] * d + [n_vec])\n    VGrid = np.zeros(grid_shape, dtype=np.float64)\n    _S2G[d](VGrid, Y_grid_2d, VScat, ng, n, n_vec)\n\n    # G2G: FFT convolution\n    PhiGrid = np.zeros(grid_shape, dtype=np.float64)\n    _CONV[d](PhiGrid, VGrid, h_exact, ng, n_vec)\n\n    # G2S: grid to scatter\n    PhiScat = np.zeros((n, n_vec), dtype=np.float64)\n    _G2S[d](PhiScat, PhiGrid, Y_grid_2d, ng, n, n_vec)\n\n    # Compute Z and repulsive forces using original-scale coords\n    Frep, zeta = _zeta_and_force(np.ascontiguousarray(Y_shifted), PhiScat, n, d)\n\n    return Frep, zeta\n</code></pre>"},{"location":"api/sgtsnepi.html","title":"<code>sgtsnepi</code> \u2014 Functional API","text":"<p>Functional API for SG-t-SNE-Pi embedding.</p>"},{"location":"api/sgtsnepi.html#pysgtsnepi.api.sgtsnepi","title":"<code>sgtsnepi(A, d=2, lambda_=10.0, max_iter=1000, early_exag=250, alpha=12.0, eta=200.0, h=0.0, Y0=None, random_state=None, unweighted_to_weighted=True)</code>","text":"<p>Embed sparse stochastic graph via SG-t-SNE-Pi.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>sparse matrix</code> <p>Adjacency or stochastic matrix (n, n). Must be square.</p> required <code>d</code> <code>int</code> <p>Embedding dimensions (1, 2, or 3).</p> <code>2</code> <code>lambda_</code> <code>float</code> <p>Rescaling parameter (default 10).</p> <code>10.0</code> <code>max_iter</code> <code>int</code> <p>Maximum iterations.</p> <code>1000</code> <code>early_exag</code> <code>int</code> <p>Early exaggeration iterations.</p> <code>250</code> <code>alpha</code> <code>float</code> <p>Exaggeration multiplier.</p> <code>12.0</code> <code>eta</code> <code>float</code> <p>Learning rate.</p> <code>200.0</code> <code>h</code> <code>float</code> <p>Grid side length for FFT. If &lt;= 0, defaults to 1.0 (matching Julia wrapper).</p> <code>0.0</code> <code>Y0</code> <code>ndarray or None</code> <p>Initial embedding of shape (n, d).</p> <code>None</code> <code>random_state</code> <code>int or None</code> <p>Random seed.</p> <code>None</code> <code>unweighted_to_weighted</code> <code>bool</code> <p>If True (default) and all edge weights are 1.0, compute Jaccard-index local weights before normalization. Matches Julia <code>flag_unweighted_to_weighted</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Y</code> <code>ndarray of shape (n, d)</code> <p>Embedding coordinates.</p> Source code in <code>src/pysgtsnepi/api.py</code> <pre><code>def sgtsnepi(\n    A,\n    d: int = 2,\n    lambda_: float = 10.0,\n    max_iter: int = 1000,\n    early_exag: int = 250,\n    alpha: float = 12.0,\n    eta: float = 200.0,\n    h: float = 0.0,\n    Y0: np.ndarray | None = None,\n    random_state: int | None = None,\n    unweighted_to_weighted: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Embed sparse stochastic graph via SG-t-SNE-Pi.\n\n    Parameters\n    ----------\n    A : sparse matrix\n        Adjacency or stochastic matrix (n, n). Must be square.\n    d : int\n        Embedding dimensions (1, 2, or 3).\n    lambda_ : float\n        Rescaling parameter (default 10).\n    max_iter : int\n        Maximum iterations.\n    early_exag : int\n        Early exaggeration iterations.\n    alpha : float\n        Exaggeration multiplier.\n    eta : float\n        Learning rate.\n    h : float\n        Grid side length for FFT. If &lt;= 0, defaults to 1.0\n        (matching Julia wrapper).\n    Y0 : ndarray or None\n        Initial embedding of shape (n, d).\n    random_state : int or None\n        Random seed.\n    unweighted_to_weighted : bool\n        If True (default) and all edge weights are 1.0, compute\n        Jaccard-index local weights before normalization. Matches\n        Julia ``flag_unweighted_to_weighted``.\n\n    Returns\n    -------\n    Y : ndarray of shape (n, d)\n        Embedding coordinates.\n    \"\"\"\n    if d not in {1, 2, 3}:\n        raise ValueError(f\"d must be 1, 2, or 3, got {d}\")\n\n    if not issparse(A):\n        raise TypeError(\"Input A must be a sparse matrix\")\n\n    P = csc_matrix(A, dtype=np.float64)\n    n = P.shape[0]\n    if P.shape[0] != P.shape[1]:\n        raise ValueError(\"Input matrix must be square\")\n\n    # Remove self-loops\n    P.setdiag(0)\n    P.eliminate_zeros()\n\n    # Convert unweighted to weighted (Jaccard index), matching Julia default\n    if unweighted_to_weighted and np.all(P.data == 1.0):\n        P = local_weights(P)\n\n    # Validate\n    if P.data.min() &lt; 0:\n        raise ValueError(\"Input matrix must have non-negative weights\")\n\n    # Track isolated nodes (zero-column)\n    col_sums = np.asarray(P.sum(axis=0)).ravel()\n    active = col_sums &gt; 0\n    n_active = active.sum()\n\n    if n_active &lt; 2:\n        raise ValueError(\"Need at least 2 connected nodes\")\n\n    # Remove isolated nodes\n    if n_active &lt; n:\n        active_idx = np.where(active)[0]\n        P = P[np.ix_(active_idx, active_idx)]\n        if Y0 is not None:\n            Y0 = Y0[active_idx]\n        col_sums = np.asarray(P.sum(axis=0)).ravel()\n\n    # Make column-stochastic\n    col_sums_safe = np.where(col_sums &gt; 0, col_sums, 1.0)\n    D_inv = 1.0 / col_sums_safe\n    P = P @ diags(D_inv)\n\n    # Lambda rescaling (C++ lambdaRescaling: -log, bisection, exp, col-normalize)\n    if lambda_ != 1.0:\n        P = lambda_rescaling(P, lambda_)\n\n    # Symmetrize: P = P + P.T (matches C++ sparsematrix.cpp)\n    P = P + P.T\n\n    # Normalize total: P /= P.sum()\n    total = P.sum()\n    if total &gt; 0:\n        P.data /= total\n\n    P = csc_matrix(P)\n\n    # Auto-select h\n    if h &lt;= 0:\n        h = 1.0\n\n    # Run embedding\n    Y = sgtsne_embedding(\n        P,\n        d=d,\n        max_iter=max_iter,\n        early_exag=early_exag,\n        alpha=alpha,\n        eta=eta,\n        h=h,\n        Y0=Y0,\n        random_state=random_state,\n    )\n\n    # Place isolated nodes\n    if n_active &lt; n:\n        Y_full = np.zeros((n, d), dtype=np.float64)\n        Y_full[np.where(active)[0]] = Y\n        # Place isolated nodes far from embedding\n        if Y.size &gt; 0:\n            corner = Y.max(axis=0) + 1.0\n            Y_full[~active] = corner\n        Y = Y_full\n\n    return Y\n</code></pre>"},{"location":"examples/01_digits_quickstart.html","title":"Quick Start: Embed Digits into 2D","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import load_digits\n\ndigits = load_digits()\nX, y = digits.data, digits.target\nprint(f\"Data shape: {X.shape}  (n_samples, n_features)\")\nprint(f\"Labels: {sorted(set(y))}\")\n</pre> from sklearn.datasets import load_digits  digits = load_digits() X, y = digits.data, digits.target print(f\"Data shape: {X.shape}  (n_samples, n_features)\") print(f\"Labels: {sorted(set(y))}\") <pre>Data shape: (1797, 64)  (n_samples, n_features)\nLabels: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n</pre> In\u00a0[2]: Copied! <pre>from pysgtsnepi import SGtSNEpi\n\nmodel = SGtSNEpi(d=2, lambda_=10, n_neighbors=15, random_state=0)\nY = model.fit_transform(X)\nprint(f\"Embedding shape: {Y.shape}\")\n</pre> from pysgtsnepi import SGtSNEpi  model = SGtSNEpi(d=2, lambda_=10, n_neighbors=15, random_state=0) Y = model.fit_transform(X) print(f\"Embedding shape: {Y.shape}\") <pre>OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre> <pre>Embedding shape: (1797, 2)\n</pre> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 8))\nscatter = ax.scatter(Y[:, 0], Y[:, 1], c=y, cmap=\"tab10\", s=5, alpha=0.7)\nax.set_aspect(\"equal\")\nax.set_title(\"SG-t-SNE-Pi embedding of sklearn digits\")\nax.set_xlabel(\"Dimension 1\")\nax.set_ylabel(\"Dimension 2\")\ncbar = fig.colorbar(scatter, ax=ax, ticks=range(10))\ncbar.set_label(\"Digit\")\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(figsize=(8, 8)) scatter = ax.scatter(Y[:, 0], Y[:, 1], c=y, cmap=\"tab10\", s=5, alpha=0.7) ax.set_aspect(\"equal\") ax.set_title(\"SG-t-SNE-Pi embedding of sklearn digits\") ax.set_xlabel(\"Dimension 1\") ax.set_ylabel(\"Dimension 2\") cbar = fig.colorbar(scatter, ax=ax, ticks=range(10)) cbar.set_label(\"Digit\") plt.tight_layout() plt.show()"},{"location":"examples/01_digits_quickstart.html#quick-start-embed-digits-into-2d","title":"Quick Start: Embed Digits into 2D\u00b6","text":"<p>This notebook demonstrates SG-t-SNE-Pi on the sklearn digits dataset (1,797 samples, 64 features). It runs in about 30 seconds and requires no external downloads.</p>"},{"location":"examples/02_mnist_embedding.html","title":"MNIST Embedding with SG-t-SNE-Pi","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml(\"mnist_784\", version=1, as_frame=False, parser=\"liac-arff\")\nX_raw, y = mnist.data.astype(np.float64), mnist.target.astype(int)\nprint(f\"Raw data shape: {X_raw.shape}\")\nprint(f\"Labels: {sorted(set(y))}\")\n</pre> import numpy as np from sklearn.datasets import fetch_openml  mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False, parser=\"liac-arff\") X_raw, y = mnist.data.astype(np.float64), mnist.target.astype(int) print(f\"Raw data shape: {X_raw.shape}\") print(f\"Labels: {sorted(set(y))}\") <pre>Raw data shape: (70000, 784)\nLabels: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n</pre> In\u00a0[2]: Copied! <pre>from skimage.feature import hog\n\ndef extract_hog_features(images, pixels_per_cell=(7, 7)):\n    \"\"\"Extract HOG features from flattened 28x28 images.\"\"\"\n    features = []\n    for img in images:\n        feat = hog(\n            img.reshape(28, 28),\n            pixels_per_cell=pixels_per_cell,\n            cells_per_block=(2, 2),\n            orientations=9,\n        )\n        features.append(feat)\n    return np.array(features)\n\nF = extract_hog_features(X_raw)\nprint(f\"HOG feature shape: {F.shape}\")\n</pre> from skimage.feature import hog  def extract_hog_features(images, pixels_per_cell=(7, 7)):     \"\"\"Extract HOG features from flattened 28x28 images.\"\"\"     features = []     for img in images:         feat = hog(             img.reshape(28, 28),             pixels_per_cell=pixels_per_cell,             cells_per_block=(2, 2),             orientations=9,         )         features.append(feat)     return np.array(features)  F = extract_hog_features(X_raw) print(f\"HOG feature shape: {F.shape}\") <pre>HOG feature shape: (70000, 324)\n</pre> In\u00a0[3]: Copied! <pre>from pysgtsnepi import SGtSNEpi\n\nmodel = SGtSNEpi(\n    d=2,\n    lambda_=10,\n    n_neighbors=30,\n    max_iter=1000,\n    early_exag=250,\n    alpha=12,\n    eta=200,\n    random_state=0,\n)\nY = model.fit_transform(F)\nprint(f\"Embedding shape: {Y.shape}\")\n</pre> from pysgtsnepi import SGtSNEpi  model = SGtSNEpi(     d=2,     lambda_=10,     n_neighbors=30,     max_iter=1000,     early_exag=250,     alpha=12,     eta=200,     random_state=0, ) Y = model.fit_transform(F) print(f\"Embedding shape: {Y.shape}\") <pre>OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n</pre> <pre>Embedding shape: (70000, 2)\n</pre> In\u00a0[4]: Copied! <pre>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import trustworthiness\n\n# kNN accuracy in embedding space\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(Y, y)\nknn_acc = knn.score(Y, y)\n\n# Silhouette score (subsample for speed)\nrng = np.random.RandomState(42)\nidx = rng.choice(len(Y), size=5000, replace=False)\nsil = silhouette_score(Y[idx], y[idx])\n\n# Trustworthiness\ntrust = trustworthiness(F[idx], Y[idx], n_neighbors=10)\n\nprint(f\"kNN accuracy (k=10): {knn_acc:.4f}\")\nprint(f\"Silhouette score (5k subsample): {sil:.4f}\")\nprint(f\"Trustworthiness (5k subsample, k=10): {trust:.4f}\")\n</pre> from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import silhouette_score from sklearn.manifold import trustworthiness  # kNN accuracy in embedding space knn = KNeighborsClassifier(n_neighbors=10) knn.fit(Y, y) knn_acc = knn.score(Y, y)  # Silhouette score (subsample for speed) rng = np.random.RandomState(42) idx = rng.choice(len(Y), size=5000, replace=False) sil = silhouette_score(Y[idx], y[idx])  # Trustworthiness trust = trustworthiness(F[idx], Y[idx], n_neighbors=10)  print(f\"kNN accuracy (k=10): {knn_acc:.4f}\") print(f\"Silhouette score (5k subsample): {sil:.4f}\") print(f\"Trustworthiness (5k subsample, k=10): {trust:.4f}\") <pre>kNN accuracy (k=10): 0.9736\nSilhouette score (5k subsample): 0.3968\nTrustworthiness (5k subsample, k=10): 0.9774\n</pre> In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(12, 12), dpi=167)\nscatter = ax.scatter(\n    Y[:, 0], Y[:, 1],\n    c=y, cmap=\"tab10\",\n    s=0.5, alpha=0.6,\n    rasterized=True,\n)\nax.set_aspect(\"equal\")\nax.set_title(\"SG-t-SNE-Pi embedding of MNIST (70k digits, HOG features)\")\nax.set_xlabel(\"Dimension 1\")\nax.set_ylabel(\"Dimension 2\")\ncbar = fig.colorbar(scatter, ax=ax, ticks=range(10), shrink=0.8)\ncbar.set_label(\"Digit\")\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(figsize=(12, 12), dpi=167) scatter = ax.scatter(     Y[:, 0], Y[:, 1],     c=y, cmap=\"tab10\",     s=0.5, alpha=0.6,     rasterized=True, ) ax.set_aspect(\"equal\") ax.set_title(\"SG-t-SNE-Pi embedding of MNIST (70k digits, HOG features)\") ax.set_xlabel(\"Dimension 1\") ax.set_ylabel(\"Dimension 2\") cbar = fig.colorbar(scatter, ax=ax, ticks=range(10), shrink=0.8) cbar.set_label(\"Digit\") plt.tight_layout() plt.show()"},{"location":"examples/02_mnist_embedding.html#mnist-embedding-with-sg-t-sne-pi","title":"MNIST Embedding with SG-t-SNE-Pi\u00b6","text":"<p>This notebook reproduces the point-cloud MNIST demo from the Julia SGtSNEpi.jl package.</p> <p>Workflow:</p> <ol> <li>Load MNIST (70,000 digits, 28\u00d728 pixels)</li> <li>Extract HOG features (324-dimensional)</li> <li>Embed into 2D with SG-t-SNE-Pi</li> <li>Evaluate and visualize</li> </ol> <p>Note: The embedding step takes ~10\u201315 minutes on a modern laptop. First run is slower due to Numba JIT compilation.</p>"},{"location":"examples/02_mnist_embedding.html#parameter-mapping","title":"Parameter Mapping\u00b6","text":"<p>The following table maps Julia SGtSNEpi.jl parameters to our Python API:</p> Julia Python Value <code>k = 3*u = 30</code> <code>n_neighbors=30</code> 30 <code>u = 10</code> <code>lambda_=10</code> 10 <code>max_iter = 1000</code> <code>max_iter=1000</code> 1000 <code>early_exag = 250</code> <code>early_exag=250</code> 250 <code>alpha = 12</code> <code>alpha=12</code> 12 <code>eta = 200</code> <code>eta=200</code> 200"},{"location":"examples/02_mnist_embedding.html#results","title":"Results\u00b6","text":"<p>The embedding should show well-separated digit clusters, comparable to the Julia reference. SG-t-SNE-Pi is designed for graph embedding (not just point clouds), so the kNN graph construction and lambda equalization steps are critical for quality.</p> <p>Comparison with Julia SGtSNEpi.jl:</p> <ul> <li>Same algorithm, same parameters, same HOG feature pipeline</li> <li>Minor numerical differences due to different kNN implementations (PyNNDescent vs. NearestNeighborDescent.jl)</li> <li>Visual quality should be equivalent</li> </ul>"},{"location":"examples/03_graph_embedding.html","title":"Graph Embedding with SG-t-SNE-Pi","text":"In\u00a0[1]: Copied! <pre>import os\nimport tarfile\nimport urllib.request\n\nimport numpy as np\nfrom scipy.io import mmread\n\nurl = \"https://suitesparse-collection-website.herokuapp.com/MM/ML_Graph/optdigits_10NN.tar.gz\"\nurllib.request.urlretrieve(url, \"optdigits_10NN.tar.gz\")\nwith tarfile.open(\"optdigits_10NN.tar.gz\") as tar:\n    tar.extractall()\nos.remove(\"optdigits_10NN.tar.gz\")\n\nA = mmread(\"optdigits_10NN/optdigits_10NN.mtx\").tocsc()\nL = np.asarray(mmread(\"optdigits_10NN/optdigits_10NN_label.mtx\")).ravel().astype(int)\nprint(f\"Graph: {A.shape[0]} nodes, {A.nnz} edges, {len(set(L))} classes\")\n</pre> import os import tarfile import urllib.request  import numpy as np from scipy.io import mmread  url = \"https://suitesparse-collection-website.herokuapp.com/MM/ML_Graph/optdigits_10NN.tar.gz\" urllib.request.urlretrieve(url, \"optdigits_10NN.tar.gz\") with tarfile.open(\"optdigits_10NN.tar.gz\") as tar:     tar.extractall() os.remove(\"optdigits_10NN.tar.gz\")  A = mmread(\"optdigits_10NN/optdigits_10NN.mtx\").tocsc() L = np.asarray(mmread(\"optdigits_10NN/optdigits_10NN_label.mtx\")).ravel().astype(int) print(f\"Graph: {A.shape[0]} nodes, {A.nnz} edges, {len(set(L))} classes\") <pre>Graph: 5620 nodes, 79650 edges, 10 classes\n</pre> In\u00a0[2]: Copied! <pre>from pysgtsnepi.api import sgtsnepi\n\nrng = np.random.default_rng(0)\nY0 = 0.01 * rng.standard_normal((A.shape[0], 2))\nY = sgtsnepi(A, d=2, Y0=Y0, random_state=0)\nprint(f\"Embedding shape: {Y.shape}\")\n</pre> from pysgtsnepi.api import sgtsnepi  rng = np.random.default_rng(0) Y0 = 0.01 * rng.standard_normal((A.shape[0], 2)) Y = sgtsnepi(A, d=2, Y0=Y0, random_state=0) print(f\"Embedding shape: {Y.shape}\") <pre>Embedding shape: (5620, 2)\n</pre> In\u00a0[3]: Copied! <pre>from sklearn.metrics import silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# kNN accuracy in embedding space\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(Y, L)\nknn_acc = knn.score(Y, L)\n\n# Silhouette score\nrng = np.random.RandomState(42)\nidx = rng.choice(len(Y), size=min(5000, len(Y)), replace=False)\nsil = silhouette_score(Y[idx], L[idx])\n\nprint(f\"kNN accuracy (k=10): {knn_acc:.4f}\")\nprint(f\"Silhouette score: {sil:.4f}\")\n</pre> from sklearn.metrics import silhouette_score from sklearn.neighbors import KNeighborsClassifier  # kNN accuracy in embedding space knn = KNeighborsClassifier(n_neighbors=10) knn.fit(Y, L) knn_acc = knn.score(Y, L)  # Silhouette score rng = np.random.RandomState(42) idx = rng.choice(len(Y), size=min(5000, len(Y)), replace=False) sil = silhouette_score(Y[idx], L[idx])  print(f\"kNN accuracy (k=10): {knn_acc:.4f}\") print(f\"Silhouette score: {sil:.4f}\") <pre>kNN accuracy (k=10): 0.9865\nSilhouette score: 0.4542\n</pre> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\nfrom pysgtsnepi.vis import show_embedding\n\nfig, ax = show_embedding(Y, L, A=A)\nax.set_title(\"SG-t-SNE-Pi embedding of optdigits_10NN graph\")\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt  from pysgtsnepi.vis import show_embedding  fig, ax = show_embedding(Y, L, A=A) ax.set_title(\"SG-t-SNE-Pi embedding of optdigits_10NN graph\") plt.tight_layout() plt.show() In\u00a0[5]: Copied! <pre>rng3 = np.random.default_rng(0)\nY0_3d = 0.01 * rng3.standard_normal((A.shape[0], 3))\nY3 = sgtsnepi(A, d=3, max_iter=500, Y0=Y0_3d, random_state=0)\nprint(f\"3D Embedding shape: {Y3.shape}\")\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\nscatter = ax.scatter(\n    Y3[:, 0], Y3[:, 1], Y3[:, 2],\n    c=L, cmap=\"tab10\",\n    s=1, alpha=0.5,\n)\nax.set_title(\"SG-t-SNE-Pi 3D embedding of optdigits_10NN\")\ncbar = fig.colorbar(scatter, ax=ax, shrink=0.6)\ncbar.set_label(\"Digit class\")\nplt.tight_layout()\nplt.show()\n</pre> rng3 = np.random.default_rng(0) Y0_3d = 0.01 * rng3.standard_normal((A.shape[0], 3)) Y3 = sgtsnepi(A, d=3, max_iter=500, Y0=Y0_3d, random_state=0) print(f\"3D Embedding shape: {Y3.shape}\")  fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection=\"3d\") scatter = ax.scatter(     Y3[:, 0], Y3[:, 1], Y3[:, 2],     c=L, cmap=\"tab10\",     s=1, alpha=0.5, ) ax.set_title(\"SG-t-SNE-Pi 3D embedding of optdigits_10NN\") cbar = fig.colorbar(scatter, ax=ax, shrink=0.6) cbar.set_label(\"Digit class\") plt.tight_layout() plt.show() <pre>3D Embedding shape: (5620, 3)\n</pre> In\u00a0[6]: Copied! <pre>import shutil\n\nshutil.rmtree(\"optdigits_10NN\", ignore_errors=True)\n</pre> import shutil  shutil.rmtree(\"optdigits_10NN\", ignore_errors=True)"},{"location":"examples/03_graph_embedding.html#graph-embedding-with-sg-t-sne-pi","title":"Graph Embedding with SG-t-SNE-Pi\u00b6","text":"<p>This notebook reproduces the intro-graph demo from the Julia SGtSNEpi.jl package.</p> <p>Key difference from the MNIST example: here we start from a pre-computed sparse graph (the <code>optdigits_10NN</code> kNN graph from the SuiteSparse Matrix Collection) \u2014 no feature extraction needed. This is SG-t-SNE-Pi's core use case: embedding any sparse graph into low dimensions.</p> <p>Workflow:</p> <ol> <li>Download the <code>optdigits_10NN</code> graph from SuiteSparse</li> <li>Embed into 2D with <code>sgtsnepi(A, d=2)</code></li> <li>Evaluate and visualize</li> <li>(Optional) Embed into 3D</li> </ol>"},{"location":"examples/03_graph_embedding.html#parameter-mapping","title":"Parameter Mapping\u00b6","text":"<p>The following table maps Julia SGtSNEpi.jl defaults to our Python <code>sgtsnepi()</code> API:</p> Julia Python Value <code>d = 2</code> <code>d=2</code> 2 <code>u = 10</code> <code>lambda_=10</code> 10 <code>max_iter = 1000</code> <code>max_iter=1000</code> 1000 <code>early_exag = 250</code> <code>early_exag=250</code> 250 <code>alpha = 12</code> <code>alpha=12</code> 12 <code>eta = 200</code> <code>eta=200</code> 200 <p>Since the input is already a graph, we call <code>sgtsnepi(A)</code> directly \u2014 no kNN construction needed.</p>"},{"location":"examples/03_graph_embedding.html#results","title":"Results\u00b6","text":"<p>The embedding should show well-separated digit clusters, comparable to the Julia reference. Unlike the MNIST point-cloud example, no feature extraction was needed \u2014 we embedded the graph directly. This demonstrates SG-t-SNE-Pi's core capability: embedding any sparse graph into low dimensions for visualization.</p>"}]}